{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connecting to Spark\n",
    "Connecting to Spark\n",
    " - Connection to spark is established by driver.\n",
    "\t- Java, Scala, Python, or R \n",
    "\t- Python or R is high-level language\n",
    "    - Python is good or R is good to write the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pyspark version - 2.4.1 is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-modules\n",
    "- Structured-Data - pyspark.sql\n",
    "- streaming data- pyspark.streaming\n",
    "- Machine learning - pyspark.mllib or pyspark.ml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark URL\n",
    "- Remote cluster using Spark URL - spark://<IP address | DNS name >:<port>\n",
    "- example : spark://13.59.151.161.7077\n",
    "- 7077 is the default-port \n",
    "- How Spark works?\n",
    "        ##### Local cluster: specify number of cores to be active or utlize.\n",
    "        - local - only 1 core;\n",
    "        - local[4] - 4 cores, or\n",
    "        - local[*] - all available cores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a local-cluster using a SparkSession builder\n",
    "spark  = SparkSession.builder \\\n",
    "            .master('local[*]') \\\n",
    "            .appName('first_spark_application') # name the application\n",
    "            .getOrCreate() # to return the existing object\n",
    "# Close connection to Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise Creating a SparkSession\n",
    "In this exercise, you'll spin up a local Spark cluster using all available cores. The cluster will be accessible via a SparkSession object.\n",
    "\n",
    "The SparkSession class has a builder attribute, which is an instance of the Builder class. The Builder class exposes three important methods that let you:\n",
    "\n",
    "specify the location of the master node;\n",
    "name the application (optional); and\n",
    "retrieve an existing SparkSession or, if there is none, create a new one.\n",
    "The SparkSession class has a version attribute which gives the version of Spark.\n",
    "\n",
    "Find out more about SparkSession here.\n",
    "\n",
    "Once you are finished with the cluster, it's a good idea to shut it down, which will free up its resources, making them available for other processes.\n",
    "\n",
    "Note:: You might find it useful to revise the slides from the lessons in the Slides panel next to the IPython Shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the PySpark module\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession object\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .appName('test') \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# What version of Spark?\n",
    "print(spark.version)\n",
    "\n",
    "# Terminate the cluster\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data-into spark\n",
    "\n",
    " - DataFrame for tabular data.\n",
    " - select methods :\n",
    "     - count() - no/. of rows\n",
    "     - show() - subset of rows\n",
    "     - printschema() - \n",
    " - Selected attributes.\n",
    "     - dtypes\n",
    "     \n",
    " - CSV formats mostly used.\n",
    " - separated by comma\n",
    "\n",
    "###### Read method in spark spark.read.csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the csv of sample data (cars)\n",
    "cars = spark.read.csv('cars.csv', header=True)\n",
    "\n",
    "# optional arguments mostly used\n",
    "# - header - is first row a header ? (default: False)\n",
    "# - sep - field separator (default : a comma ',')\n",
    "# - schema - explicit column data types.\n",
    "# - inferSchema - deduce column data types from data?\n",
    "# - nullvalue - missing data.\n",
    "\n",
    "cars.show() # the method can shows the data of the dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleansing the data using spark\n",
    "\n",
    "##### Check column type \n",
    "- cars.printSchema()\n",
    "- gives the schema of columns used in data-frame \n",
    "- This also shows the datatypes \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferring the column types from data\n",
    "cars = spark.read.csv('cars.csv', header = True, inferSchema = True)\n",
    "# By specifying True of inferschema, the datatypes of loaded columns will be exactly as the same of pre-defined dataframe or csv\n",
    "\n",
    "# figure out the dtypes.\n",
    "cars.dtypes\n",
    "\n",
    "# Handling the missing data. by specifying nullvalue = 'NA' \n",
    "# Note nullvalue is case sensitive.\n",
    "cars = spark.read.csv('cars.csv', header=True, inferschema=True, nullvalue='NA')\n",
    "\n",
    "# if the data-types cannot be inferrred. then specify them manually\n",
    "# create a data-structure \n",
    "# then remove the infer schema and replace schema with schema defined below.\n",
    "schema = StructType([\n",
    "    StructField(\"maker\", StringType())\n",
    "    StructField(\"model\", StringType())\n",
    "    StructField(\"origin\", StringType())\n",
    "    StructField(\"type\", StringType())\n",
    "    StructField(\"cyl\", StringType())\n",
    "    StructField(\"size\", StringType())\n",
    "    StructField(\"weight\", StringType())\n",
    "    StructField(\"length\", StringType())\n",
    "    StructField(\"rpm\", StringType())\n",
    "    StructField(\"consumption\", StringType())\n",
    "])\n",
    "\n",
    "cars = spark.read.csv('cars.csv', header=True, schema=schema, nullvalue='NA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise \n",
    "Loading flights data\n",
    "In this exercise you're going to load some airline flight data from a CSV file. To ensure that the exercise runs quickly these data have been trimmed down to only 50 000 records. You can get a larger dataset in the same format here.\n",
    "\n",
    "Notes on CSV format:\n",
    "\n",
    "fields are separated by a comma (this is the default separator) and\n",
    "missing data are denoted by the string 'NA'.\n",
    "Data dictionary:\n",
    "\n",
    "- mon — month (integer between 1 and 12)\n",
    "- dom — day of month (integer between 1 and 31)\n",
    "- dow — day of week (integer; 1 = Monday and 7 = Sunday)\n",
    "- org — origin airport (IATA code)\n",
    "- mile — distance (miles)\n",
    "- carrier — carrier (IATA code)\n",
    "- depart — departure time (decimal hour)\n",
    "- duration — expected duration (minutes)\n",
    "- delay — delay (minutes)\n",
    "\n",
    "* pyspark has been imported for you and the session has been initialized.\n",
    "\n",
    "Note: The data have been aggressively down-sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from CSV file\n",
    "flights = spark.read.csv('flights.csv',\n",
    "                         sep=',',\n",
    "                         header=True,\n",
    "                         inferSchema=True,\n",
    "                         nullValue='NA')\n",
    "\n",
    "# Get number of records\n",
    "print(\"The data contain %d records.\" % flights.count())\n",
    "\n",
    "# View the first five records\n",
    "flights.show(5)\n",
    "\n",
    "# Check column data types\n",
    "flights.dtypes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Output:\n",
    "The data contain 50000 records.\n",
    "+---+---+---+-------+------+---+----+------+--------+-----+\n",
    "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
    "+---+---+---+-------+------+---+----+------+--------+-----+\n",
    "| 11| 20|  6|     US|    19|JFK|2153|  9.48|     351| null|\n",
    "|  0| 22|  2|     UA|  1107|ORD| 316| 16.33|      82|   30|\n",
    "|  2| 20|  4|     UA|   226|SFO| 337|  6.17|      82|   -8|\n",
    "|  9| 13|  1|     AA|   419|ORD|1236| 10.33|     195|   -5|\n",
    "|  4|  2|  5|     AA|   325|ORD| 258|  8.92|      65| null|\n",
    "+---+---+---+-------+------+---+----+------+--------+-----+\n",
    "only showing top 5 rows\n",
    "Out[1]: \n",
    "[('mon', 'int'),\n",
    " ('dom', 'int'),\n",
    " ('dow', 'int'),\n",
    " ('carrier', 'string'),\n",
    " ('flight', 'int'),\n",
    " ('org', 'string'),\n",
    " ('mile', 'int'),\n",
    " ('depart', 'double'),\n",
    " ('duration', 'int'),\n",
    " ('delay', 'int')]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "Loading SMS spam data\n",
    "You've seen that it's possible to infer data types directly from the data. Sometimes it's convenient to have direct control over the column types. You do this by defining an explicit schema.\n",
    "\n",
    "The file sms.csv contains a selection of SMS messages which have been classified as either 'spam' or 'ham'. These data have been adapted from the UCI Machine Learning Repository. There are a total of 5574 SMS, of which 747 have been labelled as spam.\n",
    "\n",
    "Notes on CSV format:\n",
    "\n",
    "no header record and\n",
    "fields are separated by a semicolon (this is not the default separator).\n",
    "Data dictionary:\n",
    "\n",
    "id — record identifier\n",
    "text — content of SMS message\n",
    "label — spam or ham (integer; 0 = ham and 1 = spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Specify column names and types\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"text\", StringType()),\n",
    "    StructField(\"label\", IntegerType())\n",
    "])\n",
    "\n",
    "# Load data from a delimited file\n",
    "sms = spark.read.csv('sms.csv', sep=';', header=True, schema=schema)\n",
    "\n",
    "# Print schema of DataFrame\n",
    "sms.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "+---+--------------------+-----+\n",
    "| id|                text|label|\n",
    "+---+--------------------+-----+\n",
    "|  2|Dont worry. I gue...|    0|\n",
    "|  3|Call FREEPHONE 08...|    1|\n",
    "|  4|Win a 1000 cash p...|    1|\n",
    "|  5|Go until jurong p...|    0|\n",
    "|  6|Ok lar... Joking ...|    0|\n",
    "|  7|Free entry in 2 a...|    1|\n",
    "|  8|U dun say so earl...|    0|\n",
    "|  9|Nah I don't think...|    0|\n",
    "| 10|FreeMsg Hey there...|    1|\n",
    "| 11|Even my brother i...|    0|\n",
    "| 12|As per your reque...|    0|\n",
    "| 13|WINNER!! As a val...|    1|\n",
    "| 14|Had your mobile 1...|    1|\n",
    "| 15|I'm gonna be home...|    0|\n",
    "| 16|SIX chances to wi...|    1|\n",
    "| 17|URGENT! You have ...|    1|\n",
    "| 18|I've been searchi...|    0|\n",
    "| 19|I HAVE A DATE ON ...|    0|\n",
    "| 20|XXXMobileMovieClu...|    1|\n",
    "| 21|Oh k...i'm watchi...|    0|\n",
    "+---+--------------------+-----+\n",
    "\n",
    "root\n",
    " |-- id: integer (nullable = true)\n",
    " |-- text: string (nullable = true)\n",
    " |-- label: integer (nullable = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
