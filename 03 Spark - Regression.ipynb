{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding\n",
    "    - Convert the index into meaningful-mathematical values.\n",
    "    - Steps:\n",
    "        1. Create a columns for each of the levels\n",
    "        2. These new columns are called dummy variables.\n",
    "        3. Assign binary values, 1 indicates the presence of the value, where 0 indicates not-present.\n",
    "    - this is space-consuming and redundant, instead * Use Sparse values * \n",
    "    - The Sparse values representation simply records the column numbers and values of the non-zeroes.\n",
    "    - The process of creating the dummy variables in called \"One-Hot Encoding\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "onehot = OneHotEncoderEstimator(inputCols=['type_idx'], outputCols=['type_dummy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the encoded to the data\n",
    "onehot = onehot.fit(cars)\n",
    "\n",
    "# how many category levels?\n",
    "onehot.categorySizes\n",
    "\n",
    "# apply the encoder on the data as shown below\n",
    "cars = onehot.transform(cars)\n",
    "cars.select('type','type_idx', 'type_dummy').distinct().sort('type_idx').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense vs Sparse format\n",
    "    1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([1.0, 0.0, 0.0, 0.0, 0.0, 7.0, 0.0, 0.0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import DenseVector, SparseVector\n",
    "\n",
    "# Store the vector\n",
    "DenseVector([1,0,0,0,0,7,0,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(8, {0: 1.0, 5: 7.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sparse representation of storing vectors\n",
    "SparseVector(8, [0,5], [1,7])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Encoding flight origin\n",
    "\n",
    "The org column in the flights data is a categorical variable giving the airport from which a flight departs.\n",
    "\n",
    "ORD — O'Hare International Airport (Chicago)\n",
    "SFO — San Francisco International Airport\n",
    "JFK — John F Kennedy International Airport (New York)\n",
    "LGA — La Guardia Airport (New York)\n",
    "SMF — Sacramento\n",
    "SJC — San Jose\n",
    "TUS — Tucson International Airport\n",
    "OGG — Kahului (Hawaii)\n",
    "Obviously this is only a small subset of airports. Nevertheless, since this is a categorical variable, it needs to be one-hot encoded before it can be used in a regression model.\n",
    "\n",
    "The data are in a variable called flights. You have already used a string indexer to create a column of indexed values corresponding to the strings in org.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the one hot encoder class\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "\n",
    "# Create an instance of the one hot encoder\n",
    "onehot = OneHotEncoderEstimator(inputCols=['org_idx'], outputCols=['org_dummy'])\n",
    "\n",
    "# Apply the one hot encoder to the flights data\n",
    "onehot = onehot.fit(flights)\n",
    "flights_onehot = onehot.transform(flights)\n",
    "\n",
    "# Check the results\n",
    "flights_onehot.select('org', 'org_idx', 'org_dummy').distinct().sort('org_idx').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shirst_size = {'S':8,'M':18,'L':20, 'XL':7}\n",
    "gets XL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression \n",
    "    - How to build regression models to predict numerical values?\n",
    "    - Model needs to describe the average of any specified values.\n",
    "        1. Find the Residuals (the distance between the point(observed values and its' correspondent value)\n",
    "        2. Loss Function = MSE = 1/N ∑ (i = 1 to N) (yi - y_cap)^2  \n",
    "        3. Take the columns and ensemble them to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "regression  = LinearRegression(labelCol = 'consumption')\n",
    "\n",
    "# train the model using fit\n",
    "regression = regression.fit(cars_train)\n",
    "\n",
    "# predict the model using transform\n",
    "predictions  = regression.transform(cars_test)\n",
    "\n",
    "# Calcualte Root Mean Square Error (RMSE)\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Find RMSE \n",
    "RegressionEvaluator(labelCol='consumption').evaluate(predictions)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Evaluator.\n",
    "    Regression Evaluator can also calculate following metrics:\n",
    "    - mae (Mean absolute Error)\n",
    "    - r2 (R2)\n",
    "    - mse (Mean Square Error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Intercept\n",
    "regression.intercept\n",
    "regression.coefficients\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Flight duration model: Just distance\n",
    "\n",
    "In this exercise you'll build a regression model to predict flight duration (the duration column).\n",
    "\n",
    "For the moment you'll keep the model simple, including only the distance of the flight (the km column) as a predictor.\n",
    "\n",
    "The data are in flights. The first few records are displayed in the terminal. These data have also been split into training and testing sets and are available as flights_train and flights_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data and take a look at the predictions\n",
    "predictions = regression.transform(flights_test)\n",
    "predictions.select('duration', 'prediction').show(5, False)\n",
    "\n",
    "# Calculate the RMSE\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Interpreting the coefficients\n",
    "\n",
    "The linear regression model for flight duration as a function of distance takes the form\n",
    "\n",
    "  duration=α+β×distance\n",
    "\n",
    "where\n",
    "- α — intercept (component of duration which does not depend on distance) and\n",
    "- β — coefficient (rate at which duration increases as a function of distance; also called the slope).\n",
    "By looking at the coefficients of your model you will be able to infer\n",
    "\n",
    "how much of the average flight duration is actually spent on the ground and\n",
    "what the average speed is during a flight.\n",
    "The linear regression model is available as regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercept (average minutes on ground)\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Coefficients\n",
    "coefs = regression.coefficients\n",
    "print(coefs)\n",
    "\n",
    "# Average minutes per km\n",
    "minutes_per_km = regression.coefficients[0]\n",
    "print(minutes_per_km)\n",
    "\n",
    "# Average speed in km per hour\n",
    "avg_speed = 60 / minutes_per_km\n",
    "print(avg_speed)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Flight duration model: Adding origin airport\n",
    "\n",
    "Some airports are busier than others. Some airports are bigger than others too. Flights departing from large or busy airports are likely to spend more time taxiing or waiting for their takeoff slot. So it stands to reason that the duration of a flight might depend not only on the distance being covered but also the airport from which the flight departs.\n",
    "\n",
    "You are going to make the regression model a little more sophisticated by including the departure airport as a predictor.\n",
    "\n",
    "These data have been split into training and testing sets and are available as flights_train and flights_test. The origin airport, stored in the org column, has been indexed into org_idx, which in turn has been one-hot encoded into org_dummy. The first few records are displayed in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Create a regression object and train on training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Create predictions for the testing data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting coefficients\n",
    "\n",
    "Remember that origin airport, org, has eight possible values (ORD, SFO, JFK, LGA, SMF, SJC, TUS and OGG) which have been one-hot encoded to seven dummy variables in org_dummy.\n",
    "\n",
    "The values for km and org_dummy have been assembled into features, which has eight columns with sparse representation. Column indices in features are as follows:\n",
    "\n",
    "- 0 — km\n",
    "- 1 — ORD\n",
    "- 2 — SFO\n",
    "- 3 — JFK\n",
    "- 4 — LGA\n",
    "- 5 — SMF\n",
    "- 6 — SJC and\n",
    "- 7 — TUS.\n",
    "Note that OGG does not appear in this list because it is the reference level for the origin airport category.\n",
    "\n",
    "In this exercise you'll be using the intercept and coefficients attributes to interpret the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average speed in km per hour\n",
    "avg_speed_hour = 60/regression.coefficients[0]\n",
    "print(avg_speed_hour)\n",
    "\n",
    "# Average minutes on ground at OGG\n",
    "inter = regression.intercept\n",
    "print(inter)\n",
    "\n",
    "# Average minutes on ground at JFK\n",
    "avg_ground_jfk = inter + regression.coefficients[3]\n",
    "print(avg_ground_jfk)\n",
    "\n",
    "# Average minutes on ground at LGA\n",
    "avg_ground_lga = inter + regression.coefficients[4]\n",
    "print(avg_ground_lga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucketing and Engineering\n",
    "     - To make convenient use of discrete values (like age, time etc.) which can be done using buckets or bins\n",
    "     - Example: Bucketing Heights\n",
    "         - When plotted with Heights at different ages, (age vs height)\n",
    "         - as the age vary, the height vary making different buckets (the categories of similar heights, ages matching). This similarity or grouping the count is called bucketting.\n",
    "         - can be segreggated to short, average, tall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Bucketizer method from ml.feature sub-method\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Create buckets using splits\n",
    "bucketizer = Bucketizer(splits=[3500, 4500, 6000, 6500], inputCol='rpm', outputCol='rpm_bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply buckets to the data\n",
    "cars  = bucketizer.transform(cars)\n",
    "\n",
    "# the result\n",
    "bucketed.select('rpg','rpb_bin').show(5)\n",
    "\n",
    "# groupby\n",
    "cars.groupby('rpm_bin').count().show()\n",
    "\n",
    "# One-hot encoded RPB buckets\n",
    "\n",
    "# Intercept and coefficient of the model\n",
    "regression.coefficients\n",
    "regression.intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Feature Engineering\n",
    "    Operations on a single column:\n",
    "        log()\n",
    "        sqrt()\n",
    "        pow()\n",
    "    Operation on two columns:\n",
    "        product\n",
    "        ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineering density\n",
    "cars  = cars.withColumns('density_line', cars.mass / cars.length) # Linear Density\n",
    "cars  = cars.withColumns('density_line', cars.mass / cars.length**2) # Linear Area\n",
    "cars  = cars.withColumns('density_line', cars.mass / cars.length**3) # Linear cubic\n",
    "\n",
    "# The predictions are viable by doing severals samples.\n",
    "# from the above case, it is clear that, using length, height and width are predicted. using basic maths\n",
    "# for more accurracy and to know which is an exact match, we have to understand the confusion-matrix, which gives us the best value to select with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucketing departure time\n",
    "\n",
    "Time of day data are a challenge with regression models. They are also a great candidate for bucketing.\n",
    "\n",
    "In this lesson you will convert the flight departure times from numeric values between 0 (corresponding to 00:00) and 24 (corresponding to 24:00) to binned values. You'll then take those binned values and one-hot encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer, OneHotEncoderEstimator\n",
    "\n",
    "# Create buckets at 3 hour intervals through the day\n",
    "buckets = Bucketizer(splits=[0, 3, 6, 9, 12, 15, 18, 21, 24], inputCol='depart', outputCol='depart_bucket')\n",
    "\n",
    "# Bucket the departure times\n",
    "bucketed = buckets.transform(flights)\n",
    "bucketed.select('depart', 'depart_bucket').show(5)\n",
    "\n",
    "# Create a one-hot encoder\n",
    "onehot = OneHotEncoderEstimator(inputCols=['depart_bucket'], outputCols=['depart_dummy'])\n",
    "\n",
    "# One-hot encode the bucketed departure times\n",
    "flights_onehot = onehot.fit(bucketed).transform(bucketed)\n",
    "flights_onehot.select('depart', 'depart_bucket', 'depart_dummy').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flight duration model: Adding departure time\n",
    "\n",
    "In the previous exercise the departure time was bucketed and converted to dummy variables. Now you're going to include those dummy variables in a regression model for flight duration.\n",
    "\n",
    "The data are in flights. The km, org_dummy and depart_dummy columns have been assembled into features, where km is index 0, org_dummy runs from index 1 to 7 and depart_dummy from index 8 to 14.\n",
    "\n",
    "The data have been split into training and testing sets and a linear regression model, regression, has been built on the training data. Predictions have been made on the testing data and are available as predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the RMSE on testing data\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
    "\n",
    "# Average minutes on ground at OGG for flights departing between 21:00 and 24:00\n",
    "avg_eve_ogg = regression.intercept\n",
    "print(avg_eve_ogg)\n",
    "\n",
    "# Average minutes on ground at OGG for flights departing between 00:00 and 03:00\n",
    "avg_night_ogg = regression.intercept + regression.coefficients[8]\n",
    "print(avg_night_ogg)\n",
    "\n",
    "# Average minutes on ground at JFK for flights departing between 00:00 and 03:00\n",
    "avg_night_jfk = regression.intercept + regression.coefficients[3] + regression.coefficients[8]\n",
    "print(avg_night_jfk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization\n",
    "#### Features : Only a few\n",
    " - LinearRegression works on coefficients\n",
    " - few columns and many columns - suits good with Linear Regression\n",
    " - Many columns and few rows - will be much challenging\n",
    " - Parsimonous model -  one that has just the minimum required number of predictions.\n",
    " - To do so, select only best set of columns\n",
    " - MSE = 1/N ∑ (i = 0toN) (yi - yi_cap)^2\n",
    " - punished for having to many co-efficients\n",
    " - + regualization added to the MSE \n",
    " \n",
    "- Lasso Regression - absolute value of the co-efficients\n",
    "- Ridge Regression - square of the co-efficients\n",
    "- strengthe of the regularization is denoted by lambda : \n",
    "- lambda : 0 - no regularlization (standard regression)\n",
    "- lambda : infinite - complete regularization (all coefficients zero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use vector assembler \n",
    "assembler = VectorAssembler(inputCols=['mass','cyl','type_dummy','density_line','density_quad','density_cube'], outputCol = 'features')\n",
    "cars = assembler.transform(cars)\n",
    "\n",
    "# Linear Regression.\n",
    "regression = LinearRegression(labelCol = 'consumption').fit(cars_train)\n",
    "\n",
    "# RMSE on testing data, calculate RMSE\n",
    "\n",
    "# Examine the coefficients \n",
    "regression.coefficients\n",
    "\n",
    "# Ridge Regression, this can be achieved by giving elasticNetParam = 0\n",
    "ridge = LinearRegression(labelCol='consumption', elasticNetParam=0, regParam=0.1)\n",
    "ridge.fit(cars_train)\n",
    "\n",
    "# Lasso Regression, this can be achieved by giving elasticNetParam = 1\n",
    "lasso = LinearRegression(labelCol='consumption', elasticNetParam=1, regParam=0.1)\n",
    "lasso.fit(cars_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flight duration model: More features!\n",
    "\n",
    "Let's add more features to our model. This will not necessarily result in a better model. Adding some features might improve the model. Adding other features might make it worse.\n",
    "\n",
    "More features will always make the model more complicated and difficult to interpret.\n",
    "\n",
    "These are the features you'll include in the next model:\n",
    "\n",
    "km\n",
    "org (origin airport, one-hot encoded, 8 levels)\n",
    "depart (departure time, binned in 3 hour intervals, one-hot encoded, 8 levels)\n",
    "dow (departure day of week, one-hot encoded, 7 levels) and\n",
    "mon (departure month, one-hot encoded, 12 levels).\n",
    "These have been assembled into the features column, which is a sparse representation of 32 columns (remember one-hot encoding produces a number of columns which is one fewer than the number of levels).\n",
    "\n",
    "The data are available as flights, randomly split into flights_train and flights_test. The object predictions is also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Fit linear regression model to training data\n",
    "regression = LinearRegression(labelCol='duration').fit(flights_train)\n",
    "\n",
    "# Make predictions on testing data\n",
    "predictions = regression.transform(flights_test)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol='duration').evaluate(predictions)\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flight duration model: Regularisation!\n",
    "\n",
    "In the previous exercise you added more predictors to the flight duration model. The model performed well on testing data, but with so many coefficients it was difficult to interpret.\n",
    "\n",
    "In this exercise you'll use Lasso regression (regularized with a L1 penalty) to create a more parsimonious model. Many of the coefficients in the resulting model will be set to zero. This means that only a subset of the predictors actually contribute to the model. Despite the simpler model, it still produces a good RMSE on the testing data.\n",
    "\n",
    "You'll use a specific value for the regularization strength. Later you'll learn how to find the best value using cross validation.\n",
    "\n",
    "The data (same as previous exercise) are available as flights, randomly split into flights_train and flights_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Fit Lasso model (α = 1) to training data\n",
    "regression = LinearRegression(labelCol='duration', regParam=1, elasticNetParam=1).fit(flights_train)\n",
    "\n",
    "# Calculate the RMSE on testing data\n",
    "rmse = RegressionEvaluator(labelCol='duration').evaluate(regression.transform(flights_test))\n",
    "print(\"The test RMSE is\", rmse)\n",
    "\n",
    "# Look at the model coefficients\n",
    "coeffs = regression.coefficients\n",
    "print(coeffs)\n",
    "\n",
    "# Number of zero coefficients\n",
    "zero_coeff = sum([beta == 0 for beta in regression.coefficients])\n",
    "print(\"Number of ceofficients equal to 0:\", zero_coeff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
